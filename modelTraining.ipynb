{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "import re\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score,\\\n",
    "recall_score, confusion_matrix, classification_report, accuracy_score \n",
    "import nltk\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def __GetLIWC(file:str): \n",
    "\tliwc = pd.read_csv(file)\n",
    "\tliwc = liwc.rename(columns = {liwc.columns[2]:'user_id'})\n",
    "\tliwcUser = liwc.groupby('user_id').mean().reset_index()\n",
    "\tliwcUser = liwcUser.drop(['Source (A)', 'Source (D)'], axis=1)\n",
    "\treturn liwcUser\n",
    "\n",
    "def mergeFea(features, liwc, empath): \n",
    "\tfeatures = pd.read_csv(features)\n",
    "\n",
    "\t#merge features\n",
    "\tliwcUser = __GetLIWC(liwc)\n",
    "\tliwcUser2 = liwcUser.iloc[:,1::]\n",
    "\tliwcUser2.columns = [str(col) + '_liwc' for col in liwcUser2.columns]\n",
    "\tliwcUser2['user_id'] = liwcUser.user_id\n",
    "\n",
    "\tempath = pd.read_csv(empath)\n",
    "\tempath2 = empath.iloc[:,1::]\n",
    "\tempath2.columns = [str(col) + '_empath' for col in empath2.columns]\n",
    "\tempath2['user_id'] = empath.user_id\n",
    "\n",
    "\tallfea = pd.merge(features, liwcUser2, on = 'user_id', how = 'right')\n",
    "\tallfea = pd.merge(allfea, empath2, on = 'user_id', how = 'right')\n",
    "\treturn allfea\n",
    "\n",
    "def preprocess2(sent):\n",
    "    #remove punctustion\n",
    "    sent = re.sub(r'[^\\w\\s]','',str(sent))\n",
    "    words = sent.split()\n",
    "    new_words = []\n",
    "    for w in words:      \n",
    "        new_words.append(w.lower())\n",
    "        \n",
    "    return ' '.join(new_words)\n",
    "\n",
    "\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key].values.reshape(-1,1)\n",
    "    \n",
    "class ItemSelectorText(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4],\n",
       "       [0],\n",
       "       [4],\n",
       "       [1],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [5],\n",
       "       [5],\n",
       "       [0],\n",
       "       [3],\n",
       "       [5],\n",
       "       [2],\n",
       "       [5],\n",
       "       [1],\n",
       "       [1],\n",
       "       [4],\n",
       "       [5],\n",
       "       [2],\n",
       "       [0],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [4],\n",
       "       [2],\n",
       "       [5],\n",
       "       [5],\n",
       "       [3],\n",
       "       [5],\n",
       "       [0],\n",
       "       [2],\n",
       "       [3],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [3],\n",
       "       [5],\n",
       "       [1],\n",
       "       [4],\n",
       "       [5],\n",
       "       [2],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [0],\n",
       "       [5],\n",
       "       [1],\n",
       "       [4],\n",
       "       [4],\n",
       "       [5],\n",
       "       [3],\n",
       "       [5],\n",
       "       [2],\n",
       "       [4],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [2],\n",
       "       [2],\n",
       "       [3],\n",
       "       [5],\n",
       "       [4],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [4],\n",
       "       [4],\n",
       "       [2],\n",
       "       [5],\n",
       "       [5],\n",
       "       [4],\n",
       "       [2],\n",
       "       [4],\n",
       "       [5],\n",
       "       [3],\n",
       "       [2],\n",
       "       [3],\n",
       "       [4],\n",
       "       [2],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [2],\n",
       "       [3],\n",
       "       [2],\n",
       "       [2],\n",
       "       [4],\n",
       "       [2],\n",
       "       [3],\n",
       "       [5],\n",
       "       [2],\n",
       "       [4],\n",
       "       [2],\n",
       "       [4],\n",
       "       [2],\n",
       "       [4],\n",
       "       [5],\n",
       "       [1],\n",
       "       [2],\n",
       "       [3],\n",
       "       [2],\n",
       "       [3],\n",
       "       [2],\n",
       "       [4],\n",
       "       [5],\n",
       "       [2],\n",
       "       [1],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [4],\n",
       "       [4],\n",
       "       [1],\n",
       "       [5],\n",
       "       [4],\n",
       "       [0],\n",
       "       [5],\n",
       "       [5],\n",
       "       [2],\n",
       "       [3],\n",
       "       [3],\n",
       "       [1],\n",
       "       [5],\n",
       "       [2],\n",
       "       [1],\n",
       "       [5],\n",
       "       [3],\n",
       "       [5],\n",
       "       [1],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [3],\n",
       "       [2],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [1],\n",
       "       [2],\n",
       "       [3],\n",
       "       [5],\n",
       "       [4],\n",
       "       [5],\n",
       "       [3],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [4],\n",
       "       [5],\n",
       "       [5],\n",
       "       [1],\n",
       "       [3],\n",
       "       [5],\n",
       "       [4],\n",
       "       [1],\n",
       "       [3],\n",
       "       [2],\n",
       "       [2],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [2],\n",
       "       [5],\n",
       "       [2],\n",
       "       [3],\n",
       "       [1],\n",
       "       [1],\n",
       "       [4],\n",
       "       [2],\n",
       "       [4],\n",
       "       [4],\n",
       "       [3],\n",
       "       [2],\n",
       "       [5],\n",
       "       [3],\n",
       "       [0],\n",
       "       [3],\n",
       "       [5],\n",
       "       [4],\n",
       "       [5],\n",
       "       [5],\n",
       "       [3],\n",
       "       [5],\n",
       "       [5],\n",
       "       [2],\n",
       "       [4],\n",
       "       [5],\n",
       "       [2],\n",
       "       [5],\n",
       "       [3],\n",
       "       [5],\n",
       "       [3],\n",
       "       [3],\n",
       "       [5],\n",
       "       [1],\n",
       "       [5],\n",
       "       [1],\n",
       "       [0],\n",
       "       [5],\n",
       "       [4],\n",
       "       [5],\n",
       "       [2],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [2],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [2],\n",
       "       [3],\n",
       "       [2],\n",
       "       [1],\n",
       "       [5],\n",
       "       [2],\n",
       "       [3],\n",
       "       [5],\n",
       "       [5],\n",
       "       [4],\n",
       "       [5],\n",
       "       [0],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [0],\n",
       "       [1],\n",
       "       [5],\n",
       "       [3],\n",
       "       [3],\n",
       "       [4],\n",
       "       [0],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [5],\n",
       "       [4],\n",
       "       [5],\n",
       "       [5],\n",
       "       [0],\n",
       "       [1],\n",
       "       [5],\n",
       "       [4],\n",
       "       [3],\n",
       "       [2],\n",
       "       [0],\n",
       "       [5],\n",
       "       [3],\n",
       "       [2],\n",
       "       [5],\n",
       "       [3],\n",
       "       [2],\n",
       "       [2],\n",
       "       [1],\n",
       "       [2],\n",
       "       [5],\n",
       "       [3],\n",
       "       [4],\n",
       "       [5],\n",
       "       [5],\n",
       "       [1],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [3],\n",
       "       [4],\n",
       "       [3],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [2],\n",
       "       [3],\n",
       "       [2],\n",
       "       [3],\n",
       "       [5],\n",
       "       [0],\n",
       "       [5],\n",
       "       [5],\n",
       "       [0],\n",
       "       [5],\n",
       "       [4],\n",
       "       [4],\n",
       "       [2],\n",
       "       [5],\n",
       "       [2],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [2],\n",
       "       [1],\n",
       "       [3],\n",
       "       [5],\n",
       "       [4],\n",
       "       [2],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [0],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [0],\n",
       "       [2],\n",
       "       [5],\n",
       "       [4],\n",
       "       [5],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [5],\n",
       "       [1],\n",
       "       [5],\n",
       "       [4],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [2],\n",
       "       [5],\n",
       "       [0],\n",
       "       [4],\n",
       "       [5],\n",
       "       [3],\n",
       "       [5],\n",
       "       [5],\n",
       "       [4],\n",
       "       [1],\n",
       "       [5],\n",
       "       [4],\n",
       "       [4],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [4],\n",
       "       [5],\n",
       "       [2]])"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj = ItemSelector('motivations')\n",
    "obj.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = '/home/lucia/phd_work/shareTask/'\n",
    "\n",
    "# #merge features\n",
    "# features = path + 'suicideDetection/features/FreqSentiMotiTopiFea.csv'\n",
    "# liwc = path + 'suicideDetection/features/liwcSW.csv'\n",
    "# empath = path + 'suicideDetection/features/empathSW.csv'\n",
    "# allfea = mergeFea(features, liwc, empath)\n",
    "\n",
    "# #select features and split train test\n",
    "# X = allfea.iloc[:, 3:146]\n",
    "# y = allfea.raw_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = pd.read_csv(path + '/data/clpsych19_training_data/Btrain_NoNoise_SW.csv')\n",
    "y = pd.read_csv(path + '/data/clpsych19_training_data/crowd_train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#concatenate text according to user id\n",
    "X1 = X[['user_id','post_body']]\n",
    "conTex = X1.groupby(['user_id'],as_index=False).agg(lambda x : x.sum() if str(x) else ' '.join(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conTex['post_body'] = conTex['post_body'].apply(lambda x: preprocess2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "allFea = pd.read_csv(path + '/suicideDetection/features/FreqSentiMotiTopiFea.csv')\n",
    "liwc = pd.read_csv(path + '/suicideDetection/features/liwcSW.csv')\n",
    "liwc = liwc.iloc[:,np.r_[2, 8:liwc.shape[1]]]\n",
    "liwc = liwc.rename(columns = {liwc.columns[0]:'user_id'})\n",
    "liwcUser = liwc.groupby('user_id').mean().reset_index()\n",
    "liwcUser.columns = [str(col) + '_liwc' for col in liwcUser.columns]\n",
    "empath = pd.read_csv(path + '/suicideDetection/features/empathSW.csv')\n",
    "empath.columns = [str(col) + '_empath' for col in empath.columns]\n",
    "allFea = pd.merge(allFea, liwcUser, left_on ='user_id', right_on = 'user_id_liwc', how = 'left')\n",
    "allFea = pd.merge(allFea, empath, left_on ='user_id', right_on = 'user_id_empath', how = 'left')\n",
    "\n",
    "# # # #liwc\n",
    "# # # allFea\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fea = pd.merge(y, conTex, on = 'user_id')\n",
    "allFea2  = pd.merge(conTex, allFea, on = 'user_id')\n",
    "#X = fea['post_body']\n",
    "y = allFea2.raw_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(allFea2, y, test_size=0.30, random_state=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(347, 158)\n",
      "(347,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_id', 'post_body', 'Unnamed: 0', 'raw_label', 'postingFrequency',\n",
       "       'postingInterval', 'generalMoreFreq', 'generalWordCount',\n",
       "       'healthPostingFrequency', 'healthPostingInterval',\n",
       "       ...\n",
       "       'attractive_empath', 'banking_empath', 'money_empath', 'body_empath',\n",
       "       'health_empath', 'injury_empath', 'medical_emergency_empath',\n",
       "       'death_empath', 'friends_empath', 'help_empath'],\n",
       "      dtype='object', length=158)"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_liwc_data = FunctionTransformer(lambda x: x[x.columns[x.columns.to_series().str.contains('liwc')]], validate=False)\n",
    "get_empath_data = FunctionTransformer(lambda x: x[x.columns[x.columns.to_series().str.contains('empath')]], validate=False)\n",
    "\n",
    "pipe1 = Pipeline([\n",
    "    \n",
    "    ('feats', FeatureUnion([\n",
    "        ('text', Pipeline([\n",
    "            ('selector', ItemSelectorText(key='post_body')),\n",
    "            ('cv', CountVectorizer()),\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "             ])),\n",
    "        ('liwc_features', Pipeline([\n",
    "                ('selector', get_liwc_data) ])),\n",
    "#         ('empath_features', Pipeline([\n",
    "#                 ('selector', get_empath_data) ])),\n",
    "\n",
    "        ('selector1', ItemSelector(key='motivations')),\n",
    "        ('selector2', ItemSelector(key='healthPostingFrequency')),\n",
    "        ('selector3', ItemSelector(key='healthPostingInterval')),\n",
    "        ('selector4', ItemSelector(key='healthMoreFreq')),\n",
    "        ('selector5', ItemSelector(key='healthWordCount')),\n",
    "        ('selector6', ItemSelector(key='mentionMethods')),\n",
    "        ('selector7', ItemSelector(key='SWFrequency')),\n",
    "        ('selector8', ItemSelector(key='SWPostingInterval')),\n",
    "        ('selector9', ItemSelector(key='SWFreq')),\n",
    "        ('selector10', ItemSelector(key='suicide_body')),\n",
    "        ('selector11', ItemSelector(key='hopeless_body')),\n",
    "        ('selector12', ItemSelector(key='self_senti')),\n",
    "        ('selector13', ItemSelector(key='mclust')),\n",
    "             ])),\n",
    "    \n",
    "    ('clf', Pipeline([\n",
    "     ('scale', StandardScaler(with_mean=False)),\n",
    "     #('feature_selection', SelectFromModel(LinearSVC(penalty=\"l2\"))),\n",
    "    ('svm', svm.SVC()),\n",
    "         ])),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucia/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('feats', FeatureUnion(n_jobs=None,\n",
       "       transformer_list=[('text', Pipeline(memory=None,\n",
       "     steps=[('selector', ItemSelectorText(key='post_body')), ('cv', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='c... max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False))]))])"
      ]
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('feats', FeatureUnion(n_jobs=None,\n",
       "         transformer_list=[('text', Pipeline(memory=None,\n",
       "       steps=[('selector', ItemSelectorText(key='post_body')), ('cv', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "          dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "          lowercase=True, max_df=1.0, max_f...body')), ('selector12', ItemSelector(key='self_senti')), ('selector13', ItemSelector(key='mclust'))],\n",
       "         transformer_weights=None)), ('clf', Pipeline(memory=None,\n",
       "       steps=[('logit', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False))]))]"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe1.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           a       0.38      0.17      0.24        35\n",
      "           b       0.00      0.00      0.00        18\n",
      "           c       0.00      0.00      0.00        32\n",
      "           d       0.44      0.91      0.59        64\n",
      "\n",
      "   micro avg       0.43      0.43      0.43       149\n",
      "   macro avg       0.20      0.27      0.21       149\n",
      "weighted avg       0.28      0.43      0.31       149\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucia/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "pred = pipe1.predict(X_test)\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check parameter names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'logit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-307-61c432e49ad1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpipe1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'logit'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'logit'"
     ]
    }
   ],
   "source": [
    "pipe1.named_steps['logit'].get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf': Pipeline(memory=None,\n",
       "      steps=[('logit', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False))]),\n",
       " 'clf__logit': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       " 'clf__logit__C': 1.0,\n",
       " 'clf__logit__class_weight': None,\n",
       " 'clf__logit__dual': False,\n",
       " 'clf__logit__fit_intercept': True,\n",
       " 'clf__logit__intercept_scaling': 1,\n",
       " 'clf__logit__max_iter': 100,\n",
       " 'clf__logit__multi_class': 'warn',\n",
       " 'clf__logit__n_jobs': None,\n",
       " 'clf__logit__penalty': 'l2',\n",
       " 'clf__logit__random_state': None,\n",
       " 'clf__logit__solver': 'warn',\n",
       " 'clf__logit__tol': 0.0001,\n",
       " 'clf__logit__verbose': 0,\n",
       " 'clf__logit__warm_start': False,\n",
       " 'clf__memory': None,\n",
       " 'clf__steps': [('logit',\n",
       "   LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False))],\n",
       " 'feats': FeatureUnion(n_jobs=None,\n",
       "        transformer_list=[('text', Pipeline(memory=None,\n",
       "      steps=[('selector', ItemSelectorText(key='post_body')), ('cv', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_f...body')), ('selector12', ItemSelector(key='self_senti')), ('selector13', ItemSelector(key='mclust'))],\n",
       "        transformer_weights=None),\n",
       " 'feats__n_jobs': None,\n",
       " 'feats__selector1': ItemSelector(key='motivations'),\n",
       " 'feats__selector10': ItemSelector(key='suicide_body'),\n",
       " 'feats__selector10__key': 'suicide_body',\n",
       " 'feats__selector11': ItemSelector(key='hopeless_body'),\n",
       " 'feats__selector11__key': 'hopeless_body',\n",
       " 'feats__selector12': ItemSelector(key='self_senti'),\n",
       " 'feats__selector12__key': 'self_senti',\n",
       " 'feats__selector13': ItemSelector(key='mclust'),\n",
       " 'feats__selector13__key': 'mclust',\n",
       " 'feats__selector1__key': 'motivations',\n",
       " 'feats__selector2': ItemSelector(key='healthPostingFrequency'),\n",
       " 'feats__selector2__key': 'healthPostingFrequency',\n",
       " 'feats__selector3': ItemSelector(key='healthPostingInterval'),\n",
       " 'feats__selector3__key': 'healthPostingInterval',\n",
       " 'feats__selector4': ItemSelector(key='healthMoreFreq'),\n",
       " 'feats__selector4__key': 'healthMoreFreq',\n",
       " 'feats__selector5': ItemSelector(key='healthWordCount'),\n",
       " 'feats__selector5__key': 'healthWordCount',\n",
       " 'feats__selector6': ItemSelector(key='mentionMethods'),\n",
       " 'feats__selector6__key': 'mentionMethods',\n",
       " 'feats__selector7': ItemSelector(key='SWFrequency'),\n",
       " 'feats__selector7__key': 'SWFrequency',\n",
       " 'feats__selector8': ItemSelector(key='SWPostingInterval'),\n",
       " 'feats__selector8__key': 'SWPostingInterval',\n",
       " 'feats__selector9': ItemSelector(key='SWFreq'),\n",
       " 'feats__selector9__key': 'SWFreq',\n",
       " 'feats__text': Pipeline(memory=None,\n",
       "      steps=[('selector', ItemSelectorText(key='post_body')), ('cv', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), pr...y=None)), ('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True))]),\n",
       " 'feats__text__cv': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       " 'feats__text__cv__analyzer': 'word',\n",
       " 'feats__text__cv__binary': False,\n",
       " 'feats__text__cv__decode_error': 'strict',\n",
       " 'feats__text__cv__dtype': numpy.int64,\n",
       " 'feats__text__cv__encoding': 'utf-8',\n",
       " 'feats__text__cv__input': 'content',\n",
       " 'feats__text__cv__lowercase': True,\n",
       " 'feats__text__cv__max_df': 1.0,\n",
       " 'feats__text__cv__max_features': None,\n",
       " 'feats__text__cv__min_df': 1,\n",
       " 'feats__text__cv__ngram_range': (1, 1),\n",
       " 'feats__text__cv__preprocessor': None,\n",
       " 'feats__text__cv__stop_words': None,\n",
       " 'feats__text__cv__strip_accents': None,\n",
       " 'feats__text__cv__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'feats__text__cv__tokenizer': None,\n",
       " 'feats__text__cv__vocabulary': None,\n",
       " 'feats__text__memory': None,\n",
       " 'feats__text__selector': ItemSelectorText(key='post_body'),\n",
       " 'feats__text__selector__key': 'post_body',\n",
       " 'feats__text__steps': [('selector', ItemSelectorText(key='post_body')),\n",
       "  ('cv', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None)),\n",
       "  ('tfidf',\n",
       "   TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True))],\n",
       " 'feats__text__tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True),\n",
       " 'feats__text__tfidf__norm': 'l2',\n",
       " 'feats__text__tfidf__smooth_idf': True,\n",
       " 'feats__text__tfidf__sublinear_tf': False,\n",
       " 'feats__text__tfidf__use_idf': True,\n",
       " 'feats__transformer_list': [('text', Pipeline(memory=None,\n",
       "        steps=[('selector', ItemSelectorText(key='post_body')), ('cv', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), pr...y=None)), ('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True))])),\n",
       "  ('selector1', ItemSelector(key='motivations')),\n",
       "  ('selector2', ItemSelector(key='healthPostingFrequency')),\n",
       "  ('selector3', ItemSelector(key='healthPostingInterval')),\n",
       "  ('selector4', ItemSelector(key='healthMoreFreq')),\n",
       "  ('selector5', ItemSelector(key='healthWordCount')),\n",
       "  ('selector6', ItemSelector(key='mentionMethods')),\n",
       "  ('selector7', ItemSelector(key='SWFrequency')),\n",
       "  ('selector8', ItemSelector(key='SWPostingInterval')),\n",
       "  ('selector9', ItemSelector(key='SWFreq')),\n",
       "  ('selector10', ItemSelector(key='suicide_body')),\n",
       "  ('selector11', ItemSelector(key='hopeless_body')),\n",
       "  ('selector12', ItemSelector(key='self_senti')),\n",
       "  ('selector13', ItemSelector(key='mclust'))],\n",
       " 'feats__transformer_weights': None,\n",
       " 'memory': None,\n",
       " 'steps': [('feats', FeatureUnion(n_jobs=None,\n",
       "          transformer_list=[('text', Pipeline(memory=None,\n",
       "        steps=[('selector', ItemSelectorText(key='post_body')), ('cv', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_f...body')), ('selector12', ItemSelector(key='self_senti')), ('selector13', ItemSelector(key='mclust'))],\n",
       "          transformer_weights=None)), ('clf', Pipeline(memory=None,\n",
       "        steps=[('logit', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False))]))]}"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe1.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid parameter cv for estimator Pipeline(memory=None,\n     steps=[('feats', FeatureUnion(n_jobs=None,\n       transformer_list=[('text', Pipeline(memory=None,\n     steps=[('selector', ItemSelectorText(key='post_body')), ('cv', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n        dtype=<class 'numpy.int64'>, encoding='utf-8', input='c...alty='l2', random_state=None, solver='warn',\n          tol=0.0001, verbose=0, warm_start=False))]))]). Check the list of available parameters with `estimator.get_params().keys()`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-309-620378fa36af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m pipe1.set_params(cv__min_df=6, \n\u001b[0;32m----> 2\u001b[0;31m                  cv__lowercase=False).fit(X_train, y_train)\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipe1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lucia/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mset_params\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \"\"\"\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'steps'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lucia/anaconda3/lib/python3.6/site-packages/sklearn/utils/metaestimators.py\u001b[0m in \u001b[0;36m_set_params\u001b[0;34m(self, attr, **params)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_replace_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m# 3. Step parameters and other initialisation arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_BaseComposition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lucia/anaconda3/lib/python3.6/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mset_params\u001b[0;34m(self, **params)\u001b[0m\n\u001b[1;32m    211\u001b[0m                                  \u001b[0;34m'Check the list of available parameters '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m                                  \u001b[0;34m'with `estimator.get_params().keys()`.'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                                  (key, self))\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdelim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid parameter cv for estimator Pipeline(memory=None,\n     steps=[('feats', FeatureUnion(n_jobs=None,\n       transformer_list=[('text', Pipeline(memory=None,\n     steps=[('selector', ItemSelectorText(key='post_body')), ('cv', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n        dtype=<class 'numpy.int64'>, encoding='utf-8', input='c...alty='l2', random_state=None, solver='warn',\n          tol=0.0001, verbose=0, warm_start=False))]))]). Check the list of available parameters with `estimator.get_params().keys()`."
     ]
    }
   ],
   "source": [
    "pipe1.set_params(cv__min_df=6, \n",
    "                 cv__lowercase=False).fit(X_train, y_train)\n",
    "pred = pipe1.predict(X_test)\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from collections import Counter\n",
    "\n",
    "class PosTagMatrix(BaseEstimator, TransformerMixin):\n",
    "    #normalise = True - devide all values by a total number of tags in the sentence\n",
    "    #tokenizer - take a custom tokenizer function\n",
    "    def __init__(self, tokenizer=lambda x: x.split(), normalize=True):\n",
    "        self.tokenizer=tokenizer\n",
    "        self.normalize=normalize\n",
    "\n",
    "    #helper function to tokenize and count parts of speech\n",
    "    def pos_func(self, sentence):\n",
    "        return Counter(tag for word,tag in nltk.pos_tag(self.tokenizer(sentence)))\n",
    "\n",
    "    # fit() doesn't do anything, this is a transformer class\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "\n",
    "    #all the work is done here\n",
    "    def transform(self, X):\n",
    "        X_tagged = X.apply(self.pos_func).apply(pd.Series).fillna(0)\n",
    "        X_tagged['n_tokens'] = X_tagged.apply(sum, axis=1)\n",
    "        if self.normalize:\n",
    "            X_tagged = X_tagged.divide(X_tagged['n_tokens'], axis=0)\n",
    "\n",
    "        return X_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CD', 'NNS', 'RB', 'NN', 'VBD', 'CC', 'VBG', 'IN', 'DT', 'PRP$', 'VBP',\n",
       "       'JJ', 'PRP', 'VBN', 'VBZ', 'TO', 'VB', 'MD', 'RP', 'WP', 'WRB', 'JJR',\n",
       "       'EX', 'RBR', 'JJS', 'WDT', 'PDT', 'RBS', 'NNP', 'FW', '$', 'UH', 'WP$',\n",
       "       'n_tokens'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj = PosTagMatrix()\n",
    "tranTag = obj.transform(X_train)\n",
    "tranTag.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testTag = obj.transform(X_test)\n",
    "tran.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_x2 = test_x.loc[:, test_x.columns.isin(tranTag)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "pipe2 = Pipeline([\n",
    "    ('u1', FeatureUnion([\n",
    "        ('tfdif_features', Pipeline([\n",
    "            ('cv', CountVectorizer()),\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "        ])),\n",
    "        ('pos_features', Pipeline([\n",
    "            ('pos', PosTagMatrix(tokenizer=nltk.word_tokenize) ),\n",
    "        ])),\n",
    "    ])),\n",
    "    ('logit', LogisticRegression()),\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(347,)\n",
      "(149,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucia/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/lucia/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 8634 features per sample; expecting 8635",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-186-76a355e6df3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpipe2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipe2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#print(classification_report(y_test, pred))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lucia/anaconda3/lib/python3.6/site-packages/sklearn/utils/metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;31m# update the docstring of the returned function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lucia/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, **predict_params)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m                 \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpredict_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mif_delegate_has_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'_final_estimator'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lucia/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \"\"\"\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lucia/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m             raise ValueError(\"X has %d features per sample; expecting %d\"\n\u001b[0;32m--> 262\u001b[0;31m                              % (X.shape[1], n_features))\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         scores = safe_sparse_dot(X, self.coef_.T,\n",
      "\u001b[0;31mValueError\u001b[0m: X has 8634 features per sample; expecting 8635"
     ]
    }
   ],
   "source": [
    "pipe2.fit(X_train, y_train)\n",
    "pred = pipe2.predict(X_test)\n",
    "#print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
