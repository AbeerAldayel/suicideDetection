{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score,\\\n",
    "recall_score, confusion_matrix, classification_report, accuracy_score \n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sys import argv\n",
    "import gc\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import make_pipeline, Pipeline\n",
    "from imblearn.combine import SMOTEENN\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ImportantFea(X, y):\n",
    "\tX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=35)\n",
    "\t\n",
    "\tmodel = ExtraTreesClassifier(random_state = 0)\n",
    "\tmodel.fit(X_train, y_train)\n",
    "\t#print(model.feature_importances_)\n",
    "\n",
    "\tprint(\"Feature ranking:\")\n",
    "\n",
    "\timportances = model.feature_importances_\n",
    "\tstd = np.std([tree.feature_importances_ for tree in model.estimators_],\n",
    "             axis=0)\n",
    "\tindices = np.argsort(importances)[::-1]\n",
    "\tfor f in range(X.shape[1]):\n",
    "\t\tprint(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "\tfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "\tfeat_importances.nlargest(20).plot(kind='barh')\n",
    "\tplt.show()\n",
    "\n",
    "\tfea_df = pd.DataFrame(feat_importances)\n",
    "\tfea_df['features'] = fea_df.index\n",
    "\tfea_df.columns = ['importance','features']\n",
    "\tfea_df.to_csv(path + 'topFea.csv')\n",
    "\treturn fea_df\n",
    "\n",
    "def ExTreeClassifier(X,y):\n",
    "\tX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=35)\n",
    "\tsmote_enn = SMOTEENN(random_state=42)\n",
    "\trf = make_pipeline(smote_enn, StandardScaler(), ExtraTreesClassifier(random_state = 0))\n",
    "\tcv = StratifiedKFold(n_splits=5, random_state = 0)\n",
    "\n",
    "\tparameters = [{'extratreesclassifier__max_features':['auto','sqrt','log2'], 'extratreesclassifier__class_weight':['balanced'], \n",
    "\t             'extratreesclassifier__max_leaf_nodes':[10,50,100], 'extratreesclassifier__max_depth':[2,5,10,20], 'extratreesclassifier__n_estimators' : [50,100,200,300,400]}]\n",
    "\t             \n",
    "\tgrid_search_item = GridSearchCV(rf,\n",
    "\t                            param_grid = parameters,\n",
    "\t                             scoring = 'accuracy',\n",
    "\t                             cv = cv,\n",
    "\t                             n_jobs = -1)\n",
    "\n",
    "\tgrid_search = grid_search_item.fit(X_train, y_train)\n",
    "\n",
    "\tprint('Best scores and best parameters')\n",
    "\tprint(grid_search.best_score_)\n",
    "\tprint(grid_search.best_params_)\n",
    "\n",
    "\ty_true, y_pred = y_test, grid_search.predict(X_test)\n",
    "\tprint(classification_report(y_true, y_pred))\n",
    "\n",
    "\timportance = grid_search.best_estimator_.steps[2][1].feature_importances_\n",
    "\n",
    "\tfeat_importances = pd.Series(importance, index=X.columns)\n",
    "\tfeat_importances.nlargest(20).plot(kind='barh')\n",
    "\tplt.show()\n",
    "\n",
    "\tfea_df = pd.DataFrame(feat_importances)\n",
    "\tfea_df['features'] = fea_df.index\n",
    "\tfea_df.columns = ['importance','features']\n",
    "\tfea_df.to_csv(path + 'topFea.csv')\n",
    "\treturn fea_df\n",
    "\n",
    "#RF to show important features\n",
    "def RFClassifier(X,y):\n",
    "\tX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=35)\n",
    "\tsmote_enn = SMOTEENN(random_state=42)\n",
    "\trf = make_pipeline(smote_enn, StandardScaler(), RandomForestClassifier(random_state=20))\n",
    "\tcv = StratifiedKFold(n_splits=5, random_state = 0)\n",
    "\n",
    "\tparameters = [{'randomforestclassifier__max_features':['auto','sqrt','log2'], 'randomforestclassifier__class_weight':['balanced'], \n",
    "\t             'randomforestclassifier__max_leaf_nodes':[10,50,100], 'randomforestclassifier__max_depth':[2,5,10,20], 'randomforestclassifier__n_estimators' : [50,100,200,300,400]}]\n",
    "\t             \n",
    "\tgrid_search_item = GridSearchCV(rf,\n",
    "\t                            param_grid = parameters,\n",
    "\t                             scoring = 'accuracy',\n",
    "\t                             cv = cv,\n",
    "\t                             n_jobs = -1)\n",
    "\n",
    "\tgrid_search = grid_search_item.fit(X_train, y_train)\n",
    "\n",
    "\tprint('Best scores and best parameters')\n",
    "\tprint(grid_search.best_score_)\n",
    "\tprint(grid_search.best_params_)\n",
    "\n",
    "\ty_true, y_pred = y_test, grid_search.predict(X_test)\n",
    "\tprint(classification_report(y_true, y_pred))\n",
    "\n",
    "\timportance = grid_search.best_estimator_.steps[2][1].feature_importances_\n",
    "\n",
    "\tfeat_importances = pd.Series(importance, index=X.columns)\n",
    "\tfeat_importances.nlargest(20).plot(kind='barh')\n",
    "\tplt.show()\n",
    "\n",
    "\tfea_df = pd.DataFrame(feat_importances)\n",
    "\tfea_df['features'] = fea_df.index\n",
    "\tfea_df.columns = ['importance','features']\n",
    "\tfea_df.to_csv(path + 'topFea.csv')\n",
    "\treturn fea_df\n",
    "\n",
    "\n",
    "def RFfeatureSel(X_train,y_train):\n",
    "\t\n",
    "\tsmote_enn = SMOTEENN(random_state=42)\n",
    "\trf = make_pipeline(smote_enn, StandardScaler(), RandomForestClassifier(random_state=20))\n",
    "\tcv = StratifiedKFold(n_splits=5, random_state = 0)\n",
    "\n",
    "\tparameters = [{'randomforestclassifier__max_features':['auto','sqrt','log2'], 'randomforestclassifier__class_weight':['balanced'], \n",
    "\t             'randomforestclassifier__max_leaf_nodes':[10,50,100], 'randomforestclassifier__max_depth':[2,5,10,20], 'randomforestclassifier__n_estimators' : [50,100,200,300,400]}]\n",
    "\t             \n",
    "\tgrid_search_item = GridSearchCV(rf,\n",
    "\t                            param_grid = parameters,\n",
    "\t                             scoring = 'accuracy',\n",
    "\t                             cv = cv,\n",
    "\t                             n_jobs = -1)\n",
    "\n",
    "\tgrid_search = grid_search_item.fit(X_train, y_train)\n",
    "\n",
    "\tprint('Best scores and best parameters')\n",
    "\tprint(grid_search.best_score_)\n",
    "\tprint(grid_search.best_params_)\n",
    "\n",
    "\n",
    "\timportance = grid_search.best_estimator_.steps[2][1].feature_importances_\n",
    "\n",
    "\tfeat_importances = pd.Series(importance, index=X_train.columns)\n",
    "\tfeat_importances.nlargest(20).plot(kind='barh')\n",
    "\tplt.show()\n",
    "\n",
    "\tfea_df = pd.DataFrame(feat_importances)\n",
    "\tfea_df['features'] = fea_df.index\n",
    "\tfea_df.columns = ['importance','features']\n",
    "\tfea_df.to_csv(path + 'topFea.csv')\n",
    "\treturn fea_df\n",
    "\n",
    "\n",
    "def selectFea(FeaImportant, Nfea):\n",
    "    sortFea = FeaImportant.sort_values(by=['importance'],ascending=False)\n",
    "    select = sortFea[1:Nfea]   \n",
    "    return select.features.values\n",
    "\n",
    "\n",
    "def SVMclassifier(X, y, X_test2):\n",
    "\n",
    "\tX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=35)\n",
    "\n",
    "\tsmote_enn = SMOTEENN(random_state=42)\n",
    "\t#ps = PredefinedSplit(test_fold=y_test)\n",
    "\tcv = StratifiedKFold(n_splits=5, random_state = 0)\n",
    "\tsvc = make_pipeline(StandardScaler(), svm.SVC())\n",
    "\tparameters = [{'svc__kernel': ['linear', 'poly', 'rbf', 'sigmoid'], 'svc__gamma': [0.01, 0.001, 0.0001],\n",
    "\t                     'svc__C':[0.1, 0.3, 0.5, 0.7, 0.9, 1.0, 1.5, 2.0, 10] , 'svc__class_weight':['balanced']}]\n",
    "\t                   \n",
    "\tgrid_search_item = GridSearchCV(estimator = svc,\n",
    "\t                          param_grid = parameters,\n",
    "\t                           cv =  cv,\n",
    "\t                           scoring = 'accuracy',\n",
    "\t                           n_jobs = -1)\n",
    "\tgrid_search = grid_search_item.fit(X_train, y_train)\n",
    "\n",
    "\tprint('Best scores and best parameters')\n",
    "\tprint(grid_search.best_score_)\n",
    "\tprint(grid_search.best_params_)\n",
    "\n",
    "\ty_true, y_pred = y_test, grid_search.predict(X_test)\n",
    "\tprint(classification_report(y_true, y_pred))\n",
    "\n",
    "\ty_pred2 = grid_search.predict(X_test2)\n",
    "\treturn y_pred2\n",
    "\n",
    "\n",
    "\n",
    "def SVMPredictTest(X_train, y_train, X_test):\n",
    "\n",
    "\tsmote_enn = SMOTEENN(random_state=42)\n",
    "\tcv = StratifiedKFold(n_splits=5, random_state = 0)\n",
    "\tsvc = make_pipeline(smote_enn, StandardScaler(), svm.SVC())\n",
    "\tparameters = [{'svc__kernel': ['linear', 'poly', 'rbf', 'sigmoid'], 'svc__gamma': [0.01, 0.001, 0.0001],\n",
    "\t                     'svc__C':[0.1, 0.3, 0.5, 0.7, 0.9, 1.0, 1.5, 2.0, 10] , 'svc__class_weight':['balanced']}]\n",
    "\t                    \n",
    "\n",
    "\tgrid_search_item = GridSearchCV(estimator = svc,\n",
    "\t                          param_grid = parameters,\n",
    "\t                           cv =  cv,\n",
    "\t                           scoring = 'accuracy',\n",
    "\t                           n_jobs = -1)\n",
    "\tgrid_search = grid_search_item.fit(X_train, y_train)\n",
    "\n",
    "\t#y_pred = grid_search.predict(X_train)\n",
    "\t#print(classification_report(y_train, y_pred))\n",
    "\n",
    "\tprint('Best scores and best parameters')\n",
    "\tprint(grid_search.best_score_)\n",
    "\tprint(grid_search.best_params_)\n",
    "\n",
    "\ty_pred = grid_search.predict(X_test)\n",
    "\n",
    "\treturn y_pred\n",
    "\t\n",
    "\n",
    "\n",
    "def GetLIWC(file:str): \n",
    "\tliwc = pd.read_csv(file)\n",
    "\tliwc = liwc.rename(columns = {liwc.columns[2]:'user_id'})\n",
    "\tliwcUser = liwc.groupby('user_id').mean().reset_index()\n",
    "\tliwcUser = liwcUser.drop(['Source (A)', 'Source (D)'], axis=1)\n",
    "\treturn liwcUser\n",
    "\n",
    "\n",
    "def mergeFea(features, liwc, empath): \n",
    "\tfeatures = pd.read_csv(features)\n",
    "\n",
    "\t#merge features\n",
    "\tliwcUser = GetLIWC(liwc)\n",
    "\tliwcUser2 = liwcUser.iloc[:,1::]\n",
    "\tliwcUser2.columns = [str(col) + '_liwc' for col in liwcUser2.columns]\n",
    "\tliwcUser2['user_id'] = liwcUser.user_id\n",
    "\n",
    "\tempath = pd.read_csv(empath)\n",
    "\tempath2 = empath.iloc[:,1::]\n",
    "\tempath2.columns = [str(col) + '_empath' for col in empath2.columns]\n",
    "\tempath2['user_id'] = empath.user_id\n",
    "\n",
    "\tallfea = pd.merge(features, liwcUser2, on = 'user_id', how = 'right')\n",
    "\tallfea = pd.merge(allfea, empath2, on = 'user_id', how = 'right')\n",
    "\treturn allfea\n",
    "\n",
    "def getCountVect(user_idFile, countVec):\n",
    "\ttext = pd.read_csv(user_idFile)\n",
    "\tcountVect = pd.read_csv(countVec)\n",
    "\tcountVect['user_id'] = text['user_id']\n",
    "\tcountVec2 = countVect.groupby(['user_id']).mean().reset_index()\n",
    "\treturn countVec2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = '/Users/lucia/phd_work/Clpsy/'\n",
    "#path = '/home/lucia/phd_work/shareTask/'\n",
    "\n",
    "#merge features\n",
    "features = path + 'suicideDetection/features/FreqSentiMotiTopiFea.csv'\n",
    "liwc = path + 'suicideDetection/features/liwcSW.csv'\n",
    "empath = path + 'suicideDetection/features/empathSW.csv'\n",
    "allfea = mergeFea(features, liwc, empath)\n",
    "\n",
    "#select features and split train test\n",
    "X = allfea.iloc[:, 3:146]\n",
    "y = allfea.raw_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##add count vect\n",
    "\n",
    "user_idFile = path + 'data/clpsych19_training_data/Btrain_NoNoise_SW.csv'\n",
    "countVec = path + 'countVec2.csv'\n",
    "countVec2 = getCountVect(user_idFile, countVec)\n",
    "#add tfidf count vect as features\n",
    "allfea = pd.merge(allfea, countVec2, on = 'user_id', how = 'right')\n",
    "\n",
    "y = allfea.raw_label\n",
    "y = y.replace(['a', 'b', 'c', 'd'], [1, 2, 2, 2]) \n",
    "allfea2 = allfea.iloc[:, 1::]\n",
    "allfea3 = allfea2.drop(['raw_label'],axis = 1)\n",
    "#allfea3 = allfea2.drop(['Unnamed: 0_y'],axis = 1)\n",
    "allfea3 = allfea3.drop(['user_id'],axis = 1)\n",
    "#RF Select feature\n",
    "# feaImp = RFfeatureSel(allfea3,y)\n",
    "feaImp = pd.read_csv(path + 'topFea.csv')\n",
    "selectFea = selectFea(feaImp,200) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def SVMPredictTest(X_train, y_train, X_test):\n",
    "\n",
    "\tsmote_enn = SMOTEENN(random_state=42)\n",
    "\tcv = StratifiedKFold(n_splits=5, random_state = 0)\n",
    "\tsvc = make_pipeline(smote_enn, StandardScaler(), svm.SVC())\n",
    "\tparameters = [{'svc__kernel': ['linear', 'poly', 'rbf', 'sigmoid'], 'svc__gamma': [0.01, 0.001, 0.0001],\n",
    "\t                     'svc__C':[0.1, 0.3, 0.5, 0.7, 0.9, 1.0, 1.5, 2.0, 10] , 'svc__class_weight':['balanced']}]\n",
    "\t                    \n",
    "\n",
    "\tgrid_search_item = GridSearchCV(estimator = svc,\n",
    "\t                          param_grid = parameters,\n",
    "\t                           cv =  cv,\n",
    "\t                           scoring = 'accuracy',\n",
    "\t                           n_jobs = -1)\n",
    "\tgrid_search = grid_search_item.fit(X_train, y_train)\n",
    "\n",
    "\t#y_pred = grid_search.predict(X_train)\n",
    "\t#print(classification_report(y_train, y_pred))\n",
    "\n",
    "\tprint('Best scores and best parameters')\n",
    "\tprint(grid_search.best_score_)\n",
    "\tprint(grid_search.best_params_)\n",
    "\n",
    "\ty_pred = grid_search.predict(X_test)\n",
    "\n",
    "\treturn y_pred\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get test set\n",
      "trainset and testset shape (496, 181) (125, 181)\n",
      "prediction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucia/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "/Users/lucia/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best scores and best parameters\n",
      "0.8346774193548387\n",
      "{'svc__C': 2.0, 'svc__class_weight': 'balanced', 'svc__gamma': 0.0001, 'svc__kernel': 'rbf'}\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucia/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "/Users/lucia/anaconda3/lib/python3.6/site-packages/imblearn/pipeline.py:349: DataConversionWarning: Data with input dtype int64, float64, object were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n"
     ]
    }
   ],
   "source": [
    "print('get test set')\n",
    "featuresT = path + 'suicideDetection/TestFeatures/FreqSentiMotiTopiFea.csv'\n",
    "liwcT = path + 'suicideDetection/TestFeatures/liwcSW.csv'\n",
    "empathT = path + 'suicideDetection/TestFeatures/empathSW.csv'\n",
    "testFea = mergeFea(featuresT, liwcT, empathT)\n",
    "\n",
    "user_idFile = path + 'data/clpsych19_training_data/testSW.csv'\n",
    "countVec = path + 'suicideDetection/TestFeatures/countVec2.csv'\n",
    "countVec2 = getCountVect(user_idFile, countVec)\n",
    "#add tfidf count vect as features\n",
    "\n",
    "\n",
    "allfeaTest = pd.merge(testFea, countVec2, on = 'user_id', how = 'right')\n",
    "selectedFeaTest = allfeaTest.loc[:, allfeaTest.columns.isin(selectFea)]\n",
    "selectedFeaTest['user_id'] = allfeaTest.user_id\n",
    "selectedFeaTest.to_csv(path + 'suicideDetection/TestFeatures/selectFeaTest.csv')\n",
    "\n",
    "selectedFeaX = allfea2.loc[:, allfea2.columns.isin(selectedFeaTest.columns)]\n",
    "selectedFeaX['user_id'] = allfea2.user_id\n",
    "print('trainset and testset shape', selectedFeaX.shape, selectedFeaTest.shape)\n",
    "\n",
    "\n",
    "print('prediction')\n",
    "#prediction\n",
    "result = SVMPredictTest(selectedFeaX, y, selectedFeaTest)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
